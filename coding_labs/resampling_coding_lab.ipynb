{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8368150",
   "metadata": {},
   "source": [
    "# Lab: Cross-Validation\n",
    "\n",
    "This also includes a cross validation method for classification problems, but we'll get to that another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e004312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                        summarize,\n",
    "                        poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "    (cross_validate,\n",
    "    KFold,\n",
    "    ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d6d9d",
   "metadata": {},
   "source": [
    "# The Validation Set Approach\n",
    "\n",
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "We use the function `train_test_split()` to split the data into training and validation sets. As there are 392 observations, we split into two equal sets of size 196 using the argument `test_size=196`. It's generally a good idea to set a random seed when performing operations like this that contain an element of randomness, so that the results obtained can be reproduced precisely at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449537ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data(\"Auto\")\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134bfee",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac56079",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS([\"horsepower\"])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train[\"mpg\"]\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4be63",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of the `results` evaluated on the model matrix for this model created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6103741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(23.616617069669893)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid[\"mpg\"]\n",
    "valid_pred = results.predict(x_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277f89c",
   "metadata": {},
   "source": [
    "Hence our estimate for the validation MSE of the linear regression fit is 23.62.\n",
    "\n",
    "We can also estimate the validation error for higher-degree polynomial regressions. We first provide a function `evalMSE()` that takes a model string as well as a training and test set and returns the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35973df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMSE(terms,\n",
    "            response,\n",
    "            train,\n",
    "            test):\n",
    "    \n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "\n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "\n",
    "    return np.mean((y_test - test_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99499667",
   "metadata": {},
   "source": [
    "Let's use this function to estimate the validation MSE using linear quadratic and cubic fits. We use the `enumerate()` function here, which gives both the values and indices of objects as one iterates over a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39363863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=3)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly(\"horsepower\", degree)],\n",
    "                       \"mpg\",\n",
    "                       Auto_train,\n",
    "                       Auto_valid)\n",
    "    \n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94a6df",
   "metadata": {},
   "source": [
    "These error rates are 23.62, 18.76, and 18,80, respectively. If we choose a different training/validation split instead, then we can expect somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b70c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=3)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly(\"horsepower\", degree)],\n",
    "                       \"mpg\",\n",
    "                       Auto_train,\n",
    "                       Auto_valid)\n",
    "    \n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0690db2",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models start with linear, quadratic, and cubic terms are 20.76, 16.95, and 16.97, respectively.\n",
    "\n",
    "These results are consistent with our previous findings: a model that predicts `mpg` using a quadratic function of `horsepower` performs better than a model that involves only a linear function of `horsepower`, and there's no evidence of improvement in using a cubic function of `horsepower`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad0b0f",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "In theory, the cross-validation estimate can be computed for any generalized linear model. In practice, however, the simplest way to cross-validate in Python is to use `sklearn`.\n",
    "\n",
    "This is a problem which often confronts data scientists: \"I have a function to do task A, and need to feed it into something that performs task B, so that I can compute B(A(D)), where D is my data.\" When A and B don't naturally speak to each other, this requires the use of a **wrapper**. In the `ISLP` package, you're provided a wrapper, `skleanr_sm()` that enables us to easily use the cross-validation tools of `sklearn` with models fit by `statsmodels`.\n",
    "\n",
    "The class `sklearn_sm()` has as its first argument a model from `statsmodels`. It can take two additional optional arguments: `model_str` which can be used to specify a formula, and `model_args` which should be a dictionary of additional arguments used when fitting the model.\n",
    "\n",
    "Here is the wrapper in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4174bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929226)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS,\n",
    "                      MS([\"horsepower\"]))\n",
    "X, Y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc916",
   "metadata": {},
   "source": [
    "The arguments to `cross_validate()` are as follows: an object with the appropriate `fit()`, `predict()`, and `score()` methods, an array of features `X` and a response `Y`. We also included an additional argument `cv` to `cross_validate()`; specifying an integer *K* results in *K*-fold cross validation. We have provided a value corresponding to the total number of observations, which results in LOOCV. The `cross_validate()` function produces a dictionary with several components; we simply want the cross-validated test score here (MSE), which is estimated to be 24.23.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 5, computes the associated validation error, and stores it in the ith element of the vector `cv_error`. The variable `d` in the for loop corresponds to the degree of the polynomial. We begin by initializing the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d696d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.4244303 , 19.03321527])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto[\"horsepower\"])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1, 6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56134c",
   "metadata": {},
   "source": [
    "As a result, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement form using higher-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e19c19",
   "metadata": {},
   "source": [
    "Above we introduced the `outer()` method of the `np.power()` function. The `outer()` method is applied to an operation that has two arguments, such as `add()`, `min()`, or `power()`. It has two arrays as arguments, and then forms a larger array where the operation is applied to each pair of elements of the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e6c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add result:\n",
      "[[ 5  7]\n",
      " [ 7  9]\n",
      " [11 13]]\n",
      "\n",
      "Power result:\n",
      "[[   9   81]\n",
      " [  25  625]\n",
      " [  81 6561]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "print(f\"Add result:\\n{np.add.outer(A, B)}\\n\")\n",
    "print(f\"Power result:\\n{np.power.outer(A, B)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba08d2",
   "metadata": {},
   "source": [
    "In the CV example above, we used *K* = *n*, but of course we can also use *K* < *n*. The code is very similar to the above (and is significantly faster). Here we use `KFold()` to partition the data into *K* = 10 random groups. We use `random_state` to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df95ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13718373])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0)\n",
    "\n",
    "for i, d in enumerate(range(1, 6)):\n",
    "    X = np.power.outer(H, np.arange(d+1)) # Remember that `H` is the `horsepower` data as an array\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv\n",
    "    )\n",
    "    \n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050251f",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-degree polynomial terms leads to a lower test error than simply using a quadratic fit.\n",
    "\n",
    "The `cross_validate` function is flexible and can take different splitting mechanisms as an argument. For instance, one can use the `ShuffleSplit()` function to implement the validation set approach just as easily as K-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9848b02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop([\"mpg\"], axis=1),\n",
    "                         Auto[\"mpg\"],\n",
    "                         cv=validation);\n",
    "\n",
    "results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749da53",
   "metadata": {},
   "source": [
    "One can estimate the variability in the test error by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85f0880e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(23.802232661034168), np.float64(1.4218450941091916))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop([\"mpg\"], axis=1),\n",
    "                         Auto[\"mpg\"],\n",
    "                         cv=validation)\n",
    "results[\"test_score\"].mean(), results[\"test_score\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9484d",
   "metadata": {},
   "source": [
    "Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Mont Carlo variation incurred by picking different random folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ab438",
   "metadata": {},
   "source": [
    "We will learn bootstrapping after linear regression is complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin-reg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
